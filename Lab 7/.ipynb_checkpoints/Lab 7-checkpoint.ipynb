{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d96789-f952-4c12-954f-0bd3408ebd39",
   "metadata": {},
   "source": [
    "# Data Encoding (Encoding Categorical Data)\n",
    "\n",
    "- Most machine learning models only accept numerical variables, preprocessing the categorical variables becomes a necessary step. We need to convert these categorical variables to numbers such that the model is able to understand and extract valuable information.\n",
    "- The performance of a machine learning model not only depends on the model and the hyperparameters but also on how we process and feed different types of variables to the model.\n",
    "- converting categorical data not only elevates the model quality but also helps in better feature engineering.\n",
    "\n",
    "# What is categorical data?\n",
    "\n",
    "- Categorical variables are usually represented as ‘strings’ or ‘categories’ and are finite in number. Examples: city a person lives in, department a person works in, highest degree a person has, grades of a student, ....etc\n",
    "\n",
    "## Ordinal Data: The categories have an inherent order.\n",
    "\n",
    "- while encoding, one should retain the information regarding the order in which the category is provided.\n",
    "## Nominal Data: The categories do not have an inherent order.\n",
    "\n",
    "- we have to consider the presence or absence of a feature. In such a case, no notion of order is present.\n",
    "\n",
    "## Categorical Data Encoding Techniques:\n",
    "\n",
    "- Ordinal Data: Label Encoding, Oridnal Encoding\n",
    "- Nominal Data: One hot Encoding, Dummy Encoding, Frequency Encoding, Target Encoding\n",
    "\n",
    "# Label Encoding or Ordinal Encoding\n",
    "\n",
    "- We use this categorical data encoding technique when the categorical feature is ordinal. In this case, retaining the order is important. Hence encoding should reflect the sequence.\n",
    "- In Label encoding, each label is converted into an integer value.\n",
    "  - Label encoding is used on alphabetically ordered data already (grades without + or -)\n",
    "  - Could also be used on nominal data in 2 cases:\n",
    "    - X features are being used in tree-based models\n",
    "    - For the target variable (Y) because Y is not used in numerical comparisons or distance calculations.\n",
    "\n",
    "# One Hot Encoding\n",
    "\n",
    "- We use this categorical data encoding technique when the features are nominal.\n",
    "- For each level of a categorical feature, we create a new variable. Each category is mapped with a binary variable containing either 0 or 1 (0 represents the absence, and 1 represents the presence of that category)\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- Does not assume any order or distribution.(No fake ranking, The model doesn’t think one category is “bigger” than another)\n",
    "- Keeps all the information (Every category is represented clearly, Nothing is lost or merged)\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "- Not suitable for tree-based models. (Prefer single columns they can split on, One-hot creates many sparse columns, which:Makes trees deeper and Makes splits less efficient), Trees often work better with label encoding instead.\n",
    "- Problem with linear regression. (If you create N columns for N categories, they become: Perfectly correlated, causing Multicollinearity unless dummy encoding is used)\n",
    "  - Multicollinearity is a condition where two or more independent variables are highly correlated, causing instability in statistical models like linear regression.\n",
    " \n",
    "# Dummy Encoding\n",
    "\n",
    "- Dummy coding scheme is similar to one-hot encoding.\n",
    "- In the case of one-hot encoding, for N categories in a variable, it uses N binary variables. The dummy encoding is a small improvement over one-hot-encoding. Dummy encoding uses N-1 features to represent N labels/categories.\n",
    "- Fixes Linear Regression Problem of One Hot Encoding\n",
    "\n",
    "## Drawbacks of One-Hot and Dummy Encoding\n",
    "\n",
    "- Expands the feature space. (More memory, Slower training, Curse of dimensionality, Worse for large datasets (high cardinality))\n",
    "- Does not add extra information while encoding. (Encoding only reformats the data, It does NOT create new meaning or signal)\n",
    "\n",
    "# Frequency Encoding\n",
    "\n",
    "- It is a way to utilize the frequency of the categories as labels.\n",
    "- Replace the categories by the count of the observations that show that category in the dataset.\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- Straightforward to implement.\n",
    "- Does not expand the feature space.\n",
    "- Can work well with tree-based algorithms.\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "- We can lose valuable information if there are two different categories with the same amount of observations count—this is because we replace them with the same number.\n",
    "- Does not handle new categories in the test set automatically. (cannot automatically handle unseen categories in the test set because their frequencies were not observed during training.)\n",
    "- No direct relationship to the target (It only shows popularity, not usefulness)\n",
    "\n",
    "## When to Use it\n",
    "\n",
    "- Nominal data\n",
    "- High cardinality\n",
    "- Tree-based models\n",
    "- Memory is a concern\n",
    "\n",
    "## When Not to Use it\n",
    "\n",
    "- You need semantic meaning (the value carries real-world meaning or information, not just a label or count.)\n",
    "- Categories are equally frequent\n",
    "\n",
    "# Target Encoding\n",
    "\n",
    "- Replacing the category with the mean target value for that category. We start by grouping each category alone, and for each group, we calculate the mean of the target in the corresponding observations. Then we assign that mean to that category.\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- Does not expand the feature space.\n",
    "- Creates a monotonic relationship between categories and the target because categories with higher target means receive larger encoded values, preserving a consistent directional relationship with the target.\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "- May lead to overfitting. (Learn noise instead of real pattern especially when: Dataset is small and a category appears very few times)\n",
    "- Loss of information if two categories have the same mean,Two different categories can end up with the same encoded value. (Different behaviors become indistinguishable)\n",
    "\n",
    "# When to Use Each Type\n",
    "\n",
    "- For most general machine learning tasks and low cardinality data (few unique values): Start with One-Hot Encoding.\n",
    "- For classical statistics/econometrics linear models: Use Dummy Encoding to avoid multicollinearity issues.\n",
    "- For high cardinality features (many unique values): Explore Frequency Encoding (simple and fast) or Target Encoding (powerful but requires careful implementation to prevent overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a23c69-d913-4780-98b9-3cc981444dfb",
   "metadata": {},
   "source": [
    "# Missing values Imputation\n",
    "\n",
    "## What is Imputation?\n",
    "\n",
    "- Imputation is a technique used for replacing the missing data with some substitute value to retain most of the data/information of the dataset.\n",
    "- These techniques are used because removing the data from the dataset every time is not feasible and can lead to a reduction in the size of the dataset to a large extend, which not only raises concerns for biasing the dataset but also leads to incorrect analysis.\n",
    "\n",
    "## Why Imputation is Important?\n",
    "\n",
    "We use imputation because Missing data can cause:\n",
    "\n",
    "- Incompatible with most of the Python libraries used in Machine Learning: While using the libraries for ML(the most common is skLearn), they don’t have a provision to automatically handle these missing data and can lead to errors.\n",
    "- Distortion in Dataset:A huge amount of missing data can cause distortions (the data no longer reflects the true original distribution or relationships after preprocessing) in the variable distribution i.e it can increase or decrease the value of a particular category in the dataset.\n",
    "- Affects the Final Model: The missing data can cause a bias in the dataset and can lead to a faulty analysis by the model.\n",
    "\n",
    "# Types of Missing Data\n",
    "\n",
    "## Missing Completely At Random (MCAR)\n",
    "\n",
    "- When the absence of data is completely unrelated to both the observed and unobserved data. Example: missing values could occur due to technical issues like a system crash.\n",
    "\n",
    "## Missing At Random (MAR)\n",
    "\n",
    "- When the probability of being missing is the same only within groups defined by the observed data. Example: being missing is lower for younger people than for older people. the probability of missing income is related to the observed variable \"age\".\n",
    "\n",
    "## Missing Not At Random (MNAR)\n",
    "\n",
    "- When the probability of missingness is related to the value of the missing data itself. Example: Patients with severe symptoms are less likely to report their health status, leading to missing data that depends on their condition.\n",
    "\n",
    "# Imputation Techniques\n",
    "\n",
    "## Mean/Median Imputation\n",
    "\n",
    "- Mean/median imputation consists of replacing all occurrences of missing values (NA) within a variable by\n",
    "  - the mean (if the variable has a Gaussian distribution (Normal Distribution))\n",
    "  - or median (if the variable has a skewed distribution)\n",
    "- Mean/median imputation has the assumption that the data are missing completely at random (MCAR).\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Easy to implement\n",
    "- Fast way of obtaining complete datasets\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- Distortion of original variance\n",
    "- Distortion of covariance with remaining variables within the dataset\n",
    "\n",
    "## Arbitrary/Constant Value Imputation\n",
    "\n",
    "- This is an important technique used in Imputation as it can handle both the Numerical and Categorical variables. This technique states that we group the missing values in a column and assign them to a new value that is far away from the range of that column. Mostly we use values like 99999999 or -9999999 or “Missing” or “Not defined” for numerical & categorical variables.\n",
    "- Assumption is Data is not Missing At Random (MNAR).\n",
    "- The missing data is imputed with an arbitrary value that is not part of the dataset or Mean/Median/Mode of data.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Easy to implement.\n",
    "- We can use it in production.\n",
    "- It retains the importance of “missing values” if it exists.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- Can distort original variable distribution.\n",
    "- Arbitrary values can create outliers.\n",
    "- Extra caution required in selecting the Arbitrary value.\n",
    "\n",
    "## Mode/Frequent Category Imputation\n",
    "\n",
    "- This technique says to replace the missing value with the variable with the highest frequency or in simple words replacing the valueswith the Mode of that column. This technique is also referred to as Mode.\n",
    "- Assumption is Data is missing at random (MAR).\n",
    "- There is a high probability that the missing data looks like the majority of the data.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Implementation is easy.\n",
    "- We can obtain a complete dataset in very little time.\n",
    "- We can use this technique in the production model.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- The higher the percentage of missing values, the higher will be the distortion.\n",
    "- May lead to over-representation of a particular category.\n",
    "- Can distort original variable distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0f2bfe4-c743-4459-a037-700615e1e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c0ddea2-0732-410c-bdd3-713b533e7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "loanData = pd.read_csv(\"Loan_Default.csv\")\n",
    "loanData.drop(['ID', 'year'], axis = 1, inplace = True) # axis 1 to drop column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95774c55-3df0-410a-8c59-56a846a4e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalFeauters = loanData.select_dtypes(include = ['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edc17363-4b3f-43ec-acdd-374257f67052",
   "metadata": {},
   "outputs": [],
   "source": [
    "OridnalFeauters = ['age'] \n",
    "NominalFeauters = categoricalFeauters.copy()\n",
    "NominalFeauters.remove('age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9106523b-14d1-46ba-9d59-314057561a70",
   "metadata": {},
   "source": [
    "# Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc98a9a-03d1-4e03-bd8b-aa2775f3a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder()\n",
    "loanData[OrdinalFeauters] = enc.fit_transform(loanData[OrdinalFeauters]) \n",
    "\n",
    "# the fit_transform() function does two things at once: \n",
    "# fit : Learns the order of categories in each column\n",
    "# transform : Converts categories into ordered numeric values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f7449b-d5a7-41e0-b9e4-9b9af87ec766",
   "metadata": {},
   "source": [
    "# Nominal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d971d719-cce1-41f1-90a0-5ec97f3a4aa5",
   "metadata": {},
   "source": [
    "## Two Methods of One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b2398-0cb7-4a3c-9b96-256ef40f3668",
   "metadata": {},
   "source": [
    "### First Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28052edd-4ff4-4110-b709-17f076dd439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using category_encoders library\n",
    "\n",
    "loanData1 = loanData.copy()\n",
    "OHE = ce.OneHotEncoder(cols=NominalFeauters, handle_unknown='return_nan', return_df=True, use_cat_names=True)\n",
    "# creates one hot encoder\n",
    "\n",
    "# handle_unknown is used for handling unknown categories that appears during test that wasnt in training data\n",
    "# return_df returns pandas dataframe instead of numpy array\n",
    "# use_cat_names Encoded columns will include original category names\n",
    "\n",
    "encoded_categories = OHE.fit_transform(loanData1[NominalFeauters])\n",
    "\n",
    "loanData1.drop(NominalFeauters, axis=1, inplace=True)\n",
    "loanData1 = pd.concat([loanData1, encoded_categories], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d98ba-992c-4a3c-9e11-77b41641aa3c",
   "metadata": {},
   "source": [
    "### Second Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3247b316-1740-45e1-b24b-aa615639d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn library\n",
    "\n",
    "loanData2 = loanData.copy()\n",
    "OHE = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# sparse_output=False By default, OneHotEncoder returns a sparse matrix (memory-efficient).\n",
    "# Setting this to False means: Output is a dense NumPy array\n",
    "\n",
    "encoded_categories = pd.DataFrame(OHE.fit_transform(loanData2[NominalFeauters]))\n",
    "\n",
    "loanData2.drop(NominalFeauters, axis=1, inplace=True)\n",
    "loanData2 = pd.concat([loanData2, encoded_categories], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa4a43c-a531-43fa-860a-bb1eb20c7f2c",
   "metadata": {},
   "source": [
    "## Dummy Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bb89045-bade-4218-a1c4-cf6d64992f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "loanData3 = loanData.copy()\n",
    "\n",
    "encoded_categories = pd.get_dummies(data=loanData3[NominalFeauters], drop_first=True)\n",
    "\n",
    "loanData3.drop(NominalFeauters, axis=1, inplace=True)\n",
    "loanData3 = pd.concat([loanData3, encoded_categories], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2a4b53-6d0a-4caa-8ac3-73780ae0d783",
   "metadata": {},
   "source": [
    "## Frequency Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ef64e4b-7762-4e34-857f-340b9abc14c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loanData4 = loanData.copy()\n",
    "\n",
    "for c in NominalFeauters:\n",
    "    freq = loanData4[c].value_counts(normalize=True)\n",
    "    loanData4[c+'_freq'] = loanData[c].map(freq)\n",
    "\n",
    "loanData4.drop(NominalFeauters, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48680a25-7c67-4547-972f-14e3c80dc0b6",
   "metadata": {},
   "source": [
    "## Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f6438e9-ce59-4665-8215-a86e3c53bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loanData5 = loanData.copy()\n",
    "TE = ce.TargetEncoder(cols=NominalFeauters)\n",
    "loanData5 = TE.fit_transform(loanData5, loanData5['loan_amount'])\n",
    "\n",
    "#target encoder only takes one target at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae7ae4f-e280-4453-b9e7-71408c35f1df",
   "metadata": {},
   "source": [
    "# Missing Values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c972c6fb-aa63-496c-acd7-450c8cb7609c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_amount                           0\n",
       "rate_of_interest                  36439\n",
       "Interest_rate_spread              36639\n",
       "Upfront_charges                   39642\n",
       "term                                 41\n",
       "property_value                    15098\n",
       "income                             9150\n",
       "Credit_Score                          0\n",
       "age                                 200\n",
       "LTV                               15098\n",
       "Status                                0\n",
       "dtir1                             24121\n",
       "loan_limit_freq                    3344\n",
       "Gender_freq                           0\n",
       "approv_in_adv_freq                  908\n",
       "loan_type_freq                        0\n",
       "loan_purpose_freq                   134\n",
       "Credit_Worthiness_freq                0\n",
       "open_credit_freq                      0\n",
       "business_or_commercial_freq           0\n",
       "Neg_ammortization_freq              121\n",
       "interest_only_freq                    0\n",
       "lump_sum_payment_freq                 0\n",
       "construction_type_freq                0\n",
       "occupancy_type_freq                   0\n",
       "Secured_by_freq                       0\n",
       "total_units_freq                      0\n",
       "credit_type_freq                      0\n",
       "co-applicant_credit_type_freq         0\n",
       "submission_of_application_freq      200\n",
       "Region_freq                           0\n",
       "Security_Type_freq                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loanData4.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19953c7c-03d2-4179-b4f0-685b089c6cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_missing_cols = ['rate_of_interest', 'Interest_rate_spread', 'Upfront_charges', 'property_value', 'LTV', 'dtir1']\n",
    "cols_to_be_imputed = ['term', 'income', 'age', 'loan_limit_freq', 'approv_in_adv_freq', 'loan_purpose_freq', 'Neg_ammortization_freq', 'submission_of_application_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f217d64-8d0c-4421-ab60-0389818447b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns with high missing values\n",
    "\n",
    "loanData4.drop(columns=high_missing_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a477eb5-7817-4f32-8f9d-e50db91dde2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "missingRows = loanData4[loanData4[cols_to_be_imputed].isnull().any(axis=1)][cols_to_be_imputed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2efb96-461e-42b7-b246-0611ab5abe42",
   "metadata": {},
   "source": [
    "## Constant Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9df81c6c-7ff8-47bf-ad35-b8140f3fd729",
   "metadata": {},
   "outputs": [],
   "source": [
    "loanData6 = loanData4.copy()\n",
    "const = SimpleImputer(strategy='constant', fill_value = 99999)\n",
    "loanData6[cols_to_be_imputed] = const.fit_transform(loanData6[cols_to_be_imputed])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d5276-e235-481b-9dc3-facc2af2d70a",
   "metadata": {},
   "source": [
    "## Mode Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63098ff9-711a-4190-87b7-f352fd6fc74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loanData7 = loanData4.copy()\n",
    "const = SimpleImputer(strategy='most_frequent')\n",
    "loanData7[cols_to_be_imputed] = const.fit_transform(loanData6[cols_to_be_imputed])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2771582-93d1-49ad-8c62-29cc214d02e7",
   "metadata": {},
   "source": [
    "## Mean/Mediam Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4d4724b-63b6-4833-9375-132b1b2081e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mobig\\AppData\\Local\\Temp\\ipykernel_78140\\827315022.py:4: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '<bound method Series.mean of 0         360.0\n",
      "1         360.0\n",
      "2         360.0\n",
      "3         360.0\n",
      "4         360.0\n",
      "          ...  \n",
      "148665    180.0\n",
      "148666    360.0\n",
      "148667    180.0\n",
      "148668    180.0\n",
      "148669    240.0\n",
      "Name: term, Length: 148670, dtype: float64>' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  loanData8[c].fillna(loanData8[c].mean, inplace=True)\n",
      "C:\\Users\\mobig\\AppData\\Local\\Temp\\ipykernel_78140\\827315022.py:4: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '<bound method Series.mean of 0          1740.0\n",
      "1          4980.0\n",
      "2          9480.0\n",
      "3         11880.0\n",
      "4         10440.0\n",
      "           ...   \n",
      "148665     7860.0\n",
      "148666     7140.0\n",
      "148667     6900.0\n",
      "148668     7140.0\n",
      "148669     7260.0\n",
      "Name: income, Length: 148670, dtype: float64>' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  loanData8[c].fillna(loanData8[c].mean, inplace=True)\n",
      "C:\\Users\\mobig\\AppData\\Local\\Temp\\ipykernel_78140\\827315022.py:4: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '<bound method Series.mean of 0         0.931341\n",
      "1         0.931341\n",
      "2         0.931341\n",
      "3         0.931341\n",
      "4         0.931341\n",
      "            ...   \n",
      "148665    0.931341\n",
      "148666    0.931341\n",
      "148667    0.931341\n",
      "148668    0.931341\n",
      "148669    0.931341\n",
      "Name: loan_limit_freq, Length: 148670, dtype: float64>' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  loanData8[c].fillna(loanData8[c].mean, inplace=True)\n",
      "C:\\Users\\mobig\\AppData\\Local\\Temp\\ipykernel_78140\\827315022.py:4: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '<bound method Series.mean of 0         0.84339\n",
      "1         0.84339\n",
      "2         0.15661\n",
      "3         0.84339\n",
      "4         0.15661\n",
      "           ...   \n",
      "148665    0.84339\n",
      "148666    0.84339\n",
      "148667    0.84339\n",
      "148668    0.84339\n",
      "148669    0.84339\n",
      "Name: approv_in_adv_freq, Length: 148670, dtype: float64>' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  loanData8[c].fillna(loanData8[c].mean, inplace=True)\n",
      "C:\\Users\\mobig\\AppData\\Local\\Temp\\ipykernel_78140\\827315022.py:4: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '<bound method Series.mean of 0         0.232462\n",
      "1         0.232462\n",
      "2         0.232462\n",
      "3         0.368927\n",
      "4         0.232462\n",
      "            ...   \n",
      "148665    0.376569\n",
      "148666    0.232462\n",
      "148667    0.368927\n",
      "148668    0.368927\n",
      "148669    0.376569\n",
      "Name: loan_purpose_freq, Length: 148670, dtype: float64>' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  loanData8[c].fillna(loanData8[c].mean, inplace=True)\n",
      "C:\\Users\\mobig\\AppData\\Local\\Temp\\ipykernel_78140\\827315022.py:4: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '<bound method Series.mean of 0         0.898155\n",
      "1         0.898155\n",
      "2         0.101845\n",
      "3         0.898155\n",
      "4         0.898155\n",
      "            ...   \n",
      "148665    0.898155\n",
      "148666    0.898155\n",
      "148667    0.898155\n",
      "148668    0.898155\n",
      "148669    0.898155\n",
      "Name: Neg_ammortization_freq, Length: 148670, dtype: float64>' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  loanData8[c].fillna(loanData8[c].mean, inplace=True)\n",
      "C:\\Users\\mobig\\AppData\\Local\\Temp\\ipykernel_78140\\827315022.py:4: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '<bound method Series.mean of 0         0.645342\n",
      "1         0.645342\n",
      "2         0.645342\n",
      "3         0.354658\n",
      "4         0.354658\n",
      "            ...   \n",
      "148665    0.645342\n",
      "148666    0.354658\n",
      "148667    0.354658\n",
      "148668    0.645342\n",
      "148669    0.354658\n",
      "Name: submission_of_application_freq, Length: 148670, dtype: float64>' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  loanData8[c].fillna(loanData8[c].mean, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "loanData8 = loanData4.copy()\n",
    "\n",
    "for c in loanData8:\n",
    "    loanData8[c].fillna(loanData8[c].mean, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea39871c-2de9-49c9-a888-0cdb56ce56bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_amount                       0\n",
       "term                              0\n",
       "income                            0\n",
       "Credit_Score                      0\n",
       "age                               0\n",
       "Status                            0\n",
       "loan_limit_freq                   0\n",
       "Gender_freq                       0\n",
       "approv_in_adv_freq                0\n",
       "loan_type_freq                    0\n",
       "loan_purpose_freq                 0\n",
       "Credit_Worthiness_freq            0\n",
       "open_credit_freq                  0\n",
       "business_or_commercial_freq       0\n",
       "Neg_ammortization_freq            0\n",
       "interest_only_freq                0\n",
       "lump_sum_payment_freq             0\n",
       "construction_type_freq            0\n",
       "occupancy_type_freq               0\n",
       "Secured_by_freq                   0\n",
       "total_units_freq                  0\n",
       "credit_type_freq                  0\n",
       "co-applicant_credit_type_freq     0\n",
       "submission_of_application_freq    0\n",
       "Region_freq                       0\n",
       "Security_Type_freq                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loanData8.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08584614-11f5-486e-9dff-9fb61dd058d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
